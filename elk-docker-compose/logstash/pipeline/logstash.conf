input {
   
    file {
        # 设置一个日志类型，后面 filter 语法使用，多个被采集的相同类型、格斯的日志文件可以设置相同的 type
        type => "ginskeleton_nginx_access"

        # 被采集的日志路径，一定是映射到容器中的路径
        # 映射规则参见根目录的 docker-compose.yml 文件 volumes: 映射语法
        path => "/usr/share/wwwlogs/project2021/ginskeleton_nginx_access.log"

        # beginning 表示从日志开头采集; end 表示从日志结尾采集
        # 一般来说，如果有旧日志需要采集到 elasticsearch , 那么首先设置为 beginning ,等采集结束，停止服务，
        # 然后修改为 end，重启服务，后续只会采集日志每次变化的部分，否则每次日志变化都是从文件开头采集,会造成日志重复被采集的严重问题
        start_position => "end"  # 可选参数：beginning 、end

        # 采集间隔秒数，这里为 3秒
        stat_interval => "3"

        #  这里不要提前将原始数据转换为 json ，否则后面坑死你
        # codec => json
    }

    file {
        type => "ginskeleton-api"
        path => "/usr/share/wwwlogs/project2021/ginskeleton-api.log"
        start_position => "end"
        stat_interval => "3"
        codec => json
    }

    file {
        type => "ginskeleton-backend"
        path => "/usr/share/wwwlogs/project2021/ginskeleton-backend.log"
        start_position => "end"
        stat_interval => "3"
        codec => json
    }

   # 匹配nginx错误日志
   file {
        type => "nginxerr"
        path => "/usr/share/wwwlogs/project2021/error.log"
        start_position => "beginning"
        stat_interval => "3"
        codec => plain
   } 

}

# 针对每一条采集的数据进行过滤
#特别注意：target_index 对应的索引名称必须以 logstash 开头,否则会被坑死.
# mutate { add_field => { "[@metadata][target_index]" => "logstash-nginxerr-%{+YYYY-MM-dd}" } }

filter {

    if [type]  != "nginxerr"  {
        json{
            source => "message"
        }
    }

	if [type] == "ginskeleton_nginx_access" {

            mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton_nginx_access" } }

            geoip {
                source => "remote_addr"
                target => "geoip"
                database => "/usr/share/logstash/db_geoLite2_city/GeoLite2-City.mmdb"
            }

	}else if [type]=="nginxerr"{

     	   mutate { add_field => { "[@metadata][target_index]" => "logstash-nginxerr-%{+YYYY-MM-dd}" } }

    }else if [type] == "ginskeleton-backend" {

           mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton-backend-%{+YYYY-MM-dd}" } }

    }else if [type] == "ginskeleton-api" {

           mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton-api-%{+YYYY-MM-dd}" } }

    }else {

	       mutate { add_field => { "[@metadata][target_index]" => "logstash-unknowindex-%{+YYYY-MM-dd}" } }

	}

	# 将 created_at 字符串格式的日期时间转换为 es 能够识别的日期格式
    grok {
        match => ["message","%{TIMESTAMP_ISO8601:created_at}"]
    }
    date {
        match => [ "created_at","ISO8601" ]
        target => "created_at"
    }
     

    # 匹配nginx错误日志采集并转为 json 格式，基本不需要改动
    if [type]=="nginx_nginxerr" {

        grok {
         match => [ "message" , "(?<created_at>%{YEAR}[./-]%{MONTHNUM2}[./-]%{MONTHDAY} %{TIME:time2}) \[%{WORD:errLevel}] %{DATA:errMsg}, client\: %{IP:clientIp}(, server\: %{IPORHOST:server})?(, request\: \"%{DATA:request}\")?(, upstream\: \"%{DATA:upstream}\")?(, host\: \"%{DATA:host_ip}\")?(, referrer\: \"%{DATA:referrer}\")?"  ]
        }

        geoip {
            source => "clientIp"
            target => "geoip"
            database => "/usr/share/logstash/db_geoLite2_city/GeoLite2-City.mmdb"
        }

     }

    #删除一些多余字段
   mutate {
    remove_field => [ "message","@version"]
    remove_field => [ "message","tags"]
    #convert => [ "[geoip][coordinates]", "float" ]
   }

}

output {
    # 调试期间可以打开，这样logstahs 就会把日志输出到控制台，
    # 你可以通过  docker  logs   容器名称 --  since 5m , 查看容器最近5分钟的日志
    # 你也可以通过 docker-compose  logs -t --tail 50 容器服务名称 ,  查看容器最近50行的日志

     stdout { codec => rubydebug }
    # 调试结束，请屏蔽上面一行，打开 elasticsearch  { } 段代码

	# 官方说，这里每出现一个elasticsearch都是一个数据库客户端连接，建议用一个连接一次性输出多个日志内容
     elasticsearch  {
        # 172.21.0.13 表示  elasticsearch 服务器所在的ip，logstash 作为客户端需要向 es 服务器的端口提交数据
         hosts => ["http://172.21.0.13:9200"]
         index => "%{[@metadata][target_index]}"
     }

}
